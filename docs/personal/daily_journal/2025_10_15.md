# CMS inbound data pipeline. 
Talked about the current CMS Inbound File data pipelines. They're not really following the same patterns as other data engineering teams. So we're going to try to transition them right now. They're creating some dynamic tables in the raw layer and the consumption layer. And we want them to switch. They can keep using dynamic tables. We just want them to use DBT and in the ingestion steps, the way we pull the data in through the stream from AWS S3 buckets into Snowflake, they want them to use the same pattern that they've been using for other ingestion. Team was very open to using DBT. So I think I'll try to start them on that transition. There may be somebody on that team who would be anxious to do some mentoring for me. So there's Leo who's probably the more skilled in terms of dbt and more modern data architecture. There's Dave, who has solid business knowledge and analyst skills and sort of junior data engineering skills, but pretty high energy and anxious to work. There's Sam who is a little bit less passionate than the other folks on the team. But Sam also has the most business knowledge, which might be the most difficult domain to tackle. His technical knowledge is solid but mostly limited to older tech (writing stored procedures in Sybase and SQL Server, SSIS packages, and so on). Not sure who would be the best fit. I'm gonna give that some thought. I can't really take them on immediately, but maybe a month or two from now, one of them shows promise, then maybe I'll add them to my mentee list. 

# Abacus Data Source Identification 
Another data source question and answer meeting with Abacus, who is our new Interop data partner. The discussion went well, it was easier than the last ones, still just as long, but we made it through a lot more stuff. Hopefully, tomorrow we can wrap up the initial list of data sources and start talking about the details of the work. 

# EDP Provider Network Set Review
I talked with Lindsay and Rich Tallon about the provider network set difficulties we face. Rich shared some of his attempts at crosswalking the data with all the different values we need to attach to a network, which include the SAFIRE version of the network as well as the BCBS BSA version of the network as well as our own versioning of the network or consolidating the network. For instance, they have networks that are like PPO01 and PPO02 and so on that indicate a sequence of really the same network but modified. So there's like they're versioning the PPO network, but all of them are valid in the time frame that they existed in. We need quite a bit of business analyst sort of data entry work. It shouldn't be high-volume data entry work, but we do need to interface this with a few different tables and design a schema for them to update. Probably deliver that with Streamlit and Snowflake. Once we have that information, we can share it with Abacus, who also needs this information about providers and the networks they belong to. 

# DBT test environment setup
I'm here with Jason, Shreya, Brad, and Amith, and I think that's it, to reconfigure the dbt environments for our projects. The Snowflake admins have created a test environment in NorthStar, and so we created that parallel environment in dbt, configured the connections, set up a test job, set the environment variables, and double-checked our logic. When we went to run our test job, we got a permissions/ownership problem in Snowflake, and so the TVAs were going to fix that this afternoon, and we'll give it another shot tomorrow. 

# reflections
Had a hard time staying focused today. Ended up making some headway on my data modeling system. So that was good progress, but still a long way to go. Also started trying to refine my documentation prompt to be architecture focused to help me build out documentation for the EDP. Tonight I worked on my masters degree admission statement for the application and started working on a pitch for Ram to let him know about my Master's and to position myself for some AI projects at work. 


